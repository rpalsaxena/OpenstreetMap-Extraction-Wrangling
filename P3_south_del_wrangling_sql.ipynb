{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 : Data Wrangling with SQL\n",
    "$$Submitted By : Rahul  Saxena$$\n",
    "\n",
    "## Map Area:\n",
    "Location : [South Delhi, India](https://s3.amazonaws.com/mapzen.odes/ex_ciiyEL8cBe67FrpUrFy4RJdN59ihC.osm.bz2)\n",
    "\n",
    "Objective : Data Collection, Wrangling, convert to CSV, insertion into db and anlaysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Python Library import statements\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import cerberus\n",
    "import schema\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets check how many different tags are there in the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 3457,\n",
      " 'nd': 440636,\n",
      " 'node': 364336,\n",
      " 'osm': 1,\n",
      " 'relation': 944,\n",
      " 'tag': 78237,\n",
      " 'way': 66280}\n"
     ]
    }
   ],
   "source": [
    "#what tags are there and how many\n",
    "file_name = 'ex_ciiyEL8cBe67FrpUrFy4RJdN59ihC.osm' # \n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for _,elem in ET.iterparse(filename):\n",
    "        if elem.tag in tags:\n",
    "            tags[elem.tag] +=1\n",
    "        else:\n",
    "            tags[elem.tag] =1\n",
    "    return tags\n",
    "        \n",
    "def test():\n",
    "    tags = count_tags('ex_ciiyEL8cBe67FrpUrFy4RJdN59ihC.osm')\n",
    "    pprint.pprint(tags)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 76627, 'lower_colon': 1556, 'other': 50, 'problemchars': 4}\n"
     ]
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\" \\?%#$@\\,\\.\\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            if lower.search(tag.attrib['k']):\n",
    "                keys['lower'] +=1\n",
    "            elif lower_colon.search(tag.attrib['k']):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif problemchars.search(tag.attrib['k']):\n",
    "                keys[\"problemchars\"] += 1\n",
    "            else :\n",
    "                keys[\"other\"] += 1\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys\n",
    "    \n",
    "keys = process_map(file_name)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written two functions key_type() and process_map(), these two help to seggregate our key values using regex matching. We are seprating our dataset in 4 groups : lower, lower_colon, problemchars, other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE) \n",
    "\n",
    "expected = [\"Avenue\", \"Commons\", \"Court\", \"Drive\", \"Lane\", \"Parkway\", \n",
    "                         \"Place\", \"Road\", \"Square\", \"Street\", \"Trail\"]\n",
    "\n",
    "#Corrections for data related inconsistency issues, if there is a value with 'Ln'. It should be converted to Lane. This will reduce inconsistency\n",
    "mapping = {'Ave'  : 'Avenue',\n",
    "           'Dr'   : 'Drive',\n",
    "           'Ln'   : 'Lane',\n",
    "           'Pkwy' : 'Parkway',\n",
    "           'Rd'   : 'Road',\n",
    "           'Rd.'   : 'Road',\n",
    "           'St'   : 'Street',\n",
    "           'street' :\"Street\",\n",
    "           'Cir'  : \"Circle\",\n",
    "           'ave'  : 'Avenue',\n",
    "           'Hwg'  : 'Highway',\n",
    "           'Hwy'  : 'Highway',\n",
    "           'Sq'   : \"Square\",\n",
    "           'NO.'  : \"No.\",\n",
    "           'Sec'  : \"Sector\",\n",
    "           'nankpura' : \"Nanakpura\",\n",
    "           'Mg' : \"Marg\",\n",
    "           'Mr.' : \"Marg\",\n",
    "           \"Have Khas\" : \"Hauzkhas\",\n",
    "           \"Hauz Khas\" : \"Hauzkhas\",\n",
    "           \n",
    "          }\n",
    "\n",
    "def audit_street_type(street_types, street_name):     \n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name) \n",
    "\n",
    "\n",
    "def is_street_name(elem):         \n",
    "    return (elem.attrib['k'] == \"addr:street\") #Function to check value to be a street name \n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    return street_types\n",
    "\n",
    "sort_street_types = audit(file_name)\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**audit_street_type()** This function searches the i/p string for the regex. If there is a match and it is not within the \"expected\" list, add the match as a key and add the string to the set.\n",
    "\n",
    "**is_street_name()** This function checks if the key is \"addr:street\" i.e, streetname details record.\n",
    "\n",
    "**audit()** This will return a list that matches previous two functions. Using this we can understand and correct our street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10': set(['h/no 1/55 sadar bazar delhi cantt 10']),\n",
      " '110016': set(['Have Khas, New Delhi,  Delhi 110016']),\n",
      " '126': set(['Sector NO. 22, nr A- 126']),\n",
      " '6': set(['Sector 6']),\n",
      " '7': set(['Sector 7']),\n",
      " 'Bagh': set(['Mint Market Nankpura, Nr Moti Bagh']),\n",
      " 'Bhangel': set(['Bhangel']),\n",
      " 'Dwarka': set(['Dwarka', 'Sec - 19, Poket 3, Dwarka', 'Sector 11 Dwarka']),\n",
      " 'Enclave': set(['Hauz Khas Enclave',\n",
      "                 'Hauzkhas Enclave',\n",
      "                 'Safdarjung Enclave']),\n",
      " 'Estate': set(['Lodhi Estate']),\n",
      " 'Extension': set(['South Extension']),\n",
      " 'Flats': set(['Street C, Munirka DDA Flats', 'Street E, Munirka DDA Flats']),\n",
      " 'Janakpuri': set(['C Block, Janakpuri']),\n",
      " 'Janpath': set(['Janpath']),\n",
      " 'Khas': set(['Hauz Khas']),\n",
      " 'Ln': set(['Arya School Ln']),\n",
      " 'Lok': set(['Anand Lok']),\n",
      " 'Main)': set([\"Lawyer's Street, Green Park (Main)\"]),\n",
      " 'Marg': set(['Abdul Gaffar Khan Marg',\n",
      "              'Africa Avenue Marg',\n",
      "              'Amrita Shergil Marg',\n",
      "              'August Kranti Marg',\n",
      "              'Aurobindo Marg',\n",
      "              'Benito Juarez Marg',\n",
      "              'Chaudhary Dalip Singh Marg',\n",
      "              'Choudhary Hukum Chand Marg',\n",
      "              'Kasturba Gandhi Marg',\n",
      "              'Nelson Mandela Marg',\n",
      "              'Nyaya Marg',\n",
      "              'Prithviraj Marg',\n",
      "              'Rao Tula Ram Marg',\n",
      "              'Sansanwal Marg',\n",
      "              'Satsang Vihar Marg',\n",
      "              'Tees January Marg',\n",
      "              'Vinay Marg']),\n",
      " 'Market': set(['Khan Market',\n",
      "                'Naoroji Nagar Market',\n",
      "                'Sujan Singh Park, Subramania Bharti Marg,Behind Khan Market']),\n",
      " 'Munirka': set(['DDA Flats, Munirka']),\n",
      " 'NIT': set(['NIT']),\n",
      " 'Nagar': set(['Ansari Nagar',\n",
      "               'L Block, Anand Vihar, Hari Nagar',\n",
      "               'Lane E, Sarojini Nagar',\n",
      "               'Lane K, RBI Staff Quarters, Sarojini Nagar']),\n",
      " 'Park': set(['Subroto Park']),\n",
      " 'Path': set(['Shanti Path']),\n",
      " 'University': set(['Jawaharlal Nehru University']),\n",
      " 'colony': set(['Khora colony']),\n",
      " 'delhi': set(['delhi']),\n",
      " 'janakpuri': set(['janakpuri']),\n",
      " 'marg': set(['Dr. Bishamber das marg', 'Tito marg']),\n",
      " 'market': set(['Green park main market']),\n",
      " 'moti': set(['moti']),\n",
      " 'vihar': set(['shankar vihar'])}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dict(sort_street_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector 6 => Sector 6\n",
      "Amrita Shergil Marg => Amrita Shergil Marg\n",
      "Choudhary Hukum Chand Marg => Choudhary Hukum Chand Marg\n",
      "Abdul Gaffar Khan Marg => Abdul Gaffar Khan Marg\n",
      "Kasturba Gandhi Marg => Kasturba Gandhi Marg\n",
      "Vinay Marg => Vinay Marg\n",
      "Sansanwal Marg => Sansanwal Marg\n",
      "Rao Tula Ram Marg => Rao Tula Ram Marg\n",
      "Nyaya Marg => Nyaya Marg\n",
      "Prithviraj Marg => Prithviraj Marg\n",
      "August Kranti Marg => August Kranti Marg\n",
      "Africa Avenue Marg => Africa Avenue Marg\n",
      "Satsang Vihar Marg => Satsang Vihar Marg\n",
      "Chaudhary Dalip Singh Marg => Chaudhary Dalip Singh Marg\n",
      "Tees January Marg => Tees January Marg\n",
      "Nelson Mandela Marg => Nelson Mandela Marg\n",
      "Aurobindo Marg => Aurobindo Marg\n",
      "Benito Juarez Marg => Benito Juarez Marg\n",
      "Arya School Ln => Arya School Lane\n",
      "moti => moti\n",
      "Khora colony => Khora colony\n",
      "L Block, Anand Vihar, Hari Nagar => L Block, Anand Vihar, Hari Nagar\n",
      "Ansari Nagar => Ansari Nagar\n",
      "Lane K, RBI Staff Quarters, Sarojini Nagar => Lane K, RBI Staff Quarters, Sarojini Nagar\n",
      "Lane E, Sarojini Nagar => Lane E, Sarojini Nagar\n",
      "Street E, Munirka DDA Flats => Street E, Munirka DDA Flats\n",
      "Street C, Munirka DDA Flats => Street C, Munirka DDA Flats\n",
      "Hauz Khas => Hauz Khas\n",
      "Mint Market Nankpura, Nr Moti Bagh => Mint Market Nankpura, Nr Moti Bagh\n",
      "Green park main market => Green park main market\n",
      "Dwarka => Dwarka\n",
      "Sector 11 Dwarka => Sector 11 Dwarka\n",
      "Sec - 19, Poket 3, Dwarka => Sec - 19, Poket 3, Dwarka\n",
      "Dr. Bishamber das marg => Dr. Bishamber das marg\n",
      "Tito marg => Tito marg\n",
      "Hauz Khas Enclave => Hauz Khas Enclave\n",
      "Hauzkhas Enclave => Hauzkhas Enclave\n",
      "Safdarjung Enclave => Safdarjung Enclave\n",
      "Subroto Park => Subroto Park\n",
      "Lawyer's Street, Green Park (Main) => Lawyer's Street, Green Park (Main)\n",
      "Sector 7 => Sector 7\n",
      "Anand Lok => Anand Lok\n",
      "C Block, Janakpuri => C Block, Janakpuri\n",
      "Shanti Path => Shanti Path\n",
      "Bhangel => Bhangel\n",
      "Have Khas, New Delhi,  Delhi 110016 => Have Khas, New Delhi,  Delhi 110016\n",
      "South Extension => South Extension\n",
      "Janpath => Janpath\n",
      "Jawaharlal Nehru University => Jawaharlal Nehru University\n",
      "delhi => delhi\n",
      "Sector NO. 22, nr A- 126 => Sector NO. 22, nr A- 126\n",
      "janakpuri => janakpuri\n",
      "shankar vihar => shankar vihar\n",
      "h/no 1/55 sadar bazar delhi cantt 10 => h/no 1/55 sadar bazar delhi cantt 10\n",
      "Lodhi Estate => Lodhi Estate\n",
      "DDA Flats, Munirka => DDA Flats, Munirka\n",
      "NIT => NIT\n",
      "Khan Market => Khan Market\n",
      "Naoroji Nagar Market => Naoroji Nagar Market\n",
      "Sujan Singh Park, Subramania Bharti Marg,Behind Khan Market => Sujan Singh Park, Subramania Bharti Marg,Behind Khan Market\n"
     ]
    }
   ],
   "source": [
    "#Function to update values with the mapping dictionary\n",
    "def update_name(name, mapping, regex):  \n",
    "    m = regex.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping:\n",
    "            name = re.sub(regex, mapping[street_type], name) \n",
    "            return name\n",
    "    return name\n",
    "    \n",
    "\n",
    "for street_type, ways in sort_street_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping, street_type_re)\n",
    "        if better_name:\n",
    "            print name, \"=>\", better_name\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **update_name()** we have updated the street names wiht correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0206': set(['020626']),\n",
      " '110 ': set(['110 001', '110 021', '110 067']),\n",
      " '1100': set(['1100016', '1100049']),\n",
      " '2013': set(['201301'])}\n"
     ]
    }
   ],
   "source": [
    "def audit_zipcode(invalid_zipcodes, zipcode):\n",
    "    twoDigits = zipcode[0:4]\n",
    "    \n",
    "    if len(zipcode) != 6:                            #if zipcode is not of length =6, add it to invalid_zipcode dictionary\n",
    "        invalid_zipcodes[twoDigits].add(zipcode)    \n",
    "    elif not twoDigits.isdigit():                    #if zipcode is not made of just digits, add it to invalid_zipcode dictionary\n",
    "        invalid_zipcodes[twoDigits].add(zipcode)\n",
    "    \n",
    "    elif twoDigits != '1100':                       #if zipcode does not start wiht 1100, add it to invalid_zipcode dictionary\n",
    "        invalid_zipcodes[twoDigits].add(zipcode)\n",
    "        \n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zip(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    invalid_zipcodes = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_zipcode(invalid_zipcodes,tag.attrib['v'])\n",
    "    return invalid_zipcodes\n",
    "\n",
    "sort_zipcode = audit_zip(file_name)\n",
    "pprint.pprint(dict(sort_zipcode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**audit_zipcode()** This function checks the i/p zipcode with various conditions. If there is a match and it is not follow the conditions, add the match as a key and add the zipcode to the set.\n",
    "\n",
    "**is_zipcode()** This function checks if the key is \"addr:postcode\" \n",
    "\n",
    "**audit_zip()** This will return a list that matches previous two functions.Using this we can understand & correct our zipcodes.\n",
    "\n",
    "**update_zip()** This function is updating the values of zipcode with correct values if its a total error the it wil replace values with None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "020626 => None\n",
      "201301 => None\n",
      "1100016 => 110016\n",
      "1100049 => 110049\n",
      "110 001 => 110001\n",
      "110 067 => 110067\n",
      "110 021 => 110021\n"
     ]
    }
   ],
   "source": [
    "def update_zip(zipcode):\n",
    "    if zipcode[0:4] == '110 ':\n",
    "        zipcode = zipcode.replace(\" \",\"\")\n",
    "        return zipcode\n",
    "    elif zipcode[0:4] != '1100':\n",
    "        zipcode = 'None'\n",
    "        return zipcode\n",
    "    elif zipcode[0:4] == '1100' and len(zipcode) >6:\n",
    "         zipcode = zipcode.replace(\"11000\",\"1100\")\n",
    "         return zipcode\n",
    "    \n",
    "\n",
    "for street_type, ways in sort_zipcode.iteritems():\n",
    "    for name in ways:\n",
    "        if update_zip(name):\n",
    "            #sortname = update_name(name)\n",
    "            #better_name =name\n",
    "            old_name = name\n",
    "            better_name = update_zip(name)\n",
    "            name = better_name\n",
    "            print old_name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code snippet we will clean the file and seprate data. We will save data of nodes tag in nodes.csv , tags tag in nodes_tags.csv , ways tag in ways.csv, ways nodes details in ways_nodes.csv, tags tag under ways tag in ways_tags.csv\n",
    "\n",
    "**tags_clean()** function is for setting the value of id, key, value and type. Here, changes in postcode and street names are add to dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "OSM_PATH = file_name\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def tags_clean(id , tag ):\n",
    "    node_tagss = {}\n",
    "    node_tagss['id'] = int(id)\n",
    "    if tag.attrib['k'] == \"addr:street\":\n",
    "        node_tagss['value'] = update_name(tag.attrib['v'], mapping, street_type_re)\n",
    "    elif tag.attrib['k'] == \"addr:postcode\":\n",
    "        node_tagss['value'] = update_zip(tag.attrib['v'])\n",
    "    else:\n",
    "        node_tagss['value'] = tag.attrib['v']\n",
    "\n",
    "    if \":\" not in tag.attrib['k']:\n",
    "        node_tagss['key'] = tag.attrib['k']\n",
    "        node_tagss['type'] = 'regular'\n",
    "        \n",
    "    else:\n",
    "        pcolon = tag.attrib['k'].index(\":\") + 1\n",
    "        node_tagss['key'] = tag.attrib['k'][pcolon:]\n",
    "        node_tagss['type'] = tag.attrib['k'][:pcolon - 1]\n",
    "        \n",
    "    \n",
    "    return node_tagss\n",
    "    \n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    node_tagss = {}\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        for node in NODE_FIELDS:\n",
    "            node_attribs[node] = element.attrib[node]\n",
    "        node_attribs['id']= int(node_attribs['id'])\n",
    "        node_attribs['uid']= int(node_attribs['uid'])\n",
    "        node_attribs['changeset']= int(node_attribs['changeset'])\n",
    "        \n",
    "        node_attribs['lon']= float(node_attribs['lon'])\n",
    "        node_attribs['lat']= float(node_attribs['lat'])\n",
    "  \n",
    "        for tag in element.iter(\"tag\"):\n",
    "            tag_clean ={}\n",
    "            if PROBLEMCHARS.search(tag.attrib['k']) == None:\n",
    "                node_tagss = tags_clean(node_attribs['id'] , tag )\n",
    "                tags.append(node_tagss)\n",
    "        \n",
    "            #tag_clean = clean(tag)\n",
    "            #if tag_clean:\n",
    "            #    tags.append(tag_clean)\n",
    "        if node_attribs:\n",
    "            return {'node': node_attribs, 'node_tags': tags}\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for way in WAY_FIELDS:\n",
    "            way_attribs[way] = element.attrib[way]\n",
    "        \n",
    "        way_attribs['id']= int(way_attribs['id'])\n",
    "        way_attribs['uid']= int(way_attribs['uid'])\n",
    "        way_attribs['changeset']= int(way_attribs['changeset'])\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            tag_clean ={}\n",
    "            if PROBLEMCHARS.search(tag.attrib['k']) == None:\n",
    "                node_tagss = tags_clean(way_attribs['id'] , tag )\n",
    "                tags.append(node_tagss)\n",
    "        \n",
    "            #tag_clean = clean(tag)\n",
    "            #if tag_clean:\n",
    "            #    tags.append(tag_clean)\n",
    "                \n",
    "        count =0\n",
    "        for nodes in element.iter(\"nd\"):\n",
    "            wnd = {}\n",
    "            wnd['id'] = int(way_attribs['id'])\n",
    "            wnd['node_id'] = int(nodes.attrib['ref'])\n",
    "            wnd['position'] = count\n",
    "            count += 1\n",
    "            \n",
    "            way_nodes.append(wnd)\n",
    "            \n",
    "        if way_attribs:\n",
    "            return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)\n",
    "    print \"Done\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have 5 csv files containing wrangled data. We can perform analysis on this data set by importing these to our sqlite database. In the next section, we have performed some analysis using sqlite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview:\n",
    "\n",
    "**File Sizes**\n",
    "* osm file       : 75.0 MB\n",
    "* nodes.csv      : 29.1 MB\n",
    "* nodes_tags.csv : 139  KB\n",
    "* ways.csv       : 3.87 MB\n",
    "* ways_nodes.csv : 10.5 MB\n",
    "* ways_tags.csv  : 2.33 MB\n",
    "\n",
    "\n",
    "**Number of nodes**\n",
    "```\n",
    "sqlite> SELECT Count(*)\n",
    "   ...> FROM   nodes;\n",
    "364336\n",
    "```\n",
    "\n",
    "**Number of ways**\n",
    "```\n",
    "sqlite> SELECT Count(*)\n",
    "   ...> FROM ways;\n",
    "66280\n",
    "```\n",
    "\n",
    "**Number of users**\n",
    "```\n",
    "sqlite> select count(distinct(user.uid))\n",
    "   ...> from (select uid from nodes\n",
    "   ...> union all\n",
    "   ...> select uid from ways) user;\n",
    "375\n",
    "```\n",
    "\n",
    "**Top 5 Active users**\n",
    "```\n",
    "sqlite> select  user.user, count(*)\n",
    "   ...> from (select user from nodes\n",
    "   ...> union all\n",
    "   ...> select user from ways) user\n",
    "   ...> group by user.user\n",
    "   ...> order by count(*) DESC\n",
    "   ...> limit 5;\n",
    "bindhu|38740\n",
    "sdivya|38651\n",
    "vamshikrishna|33491\n",
    "Ashok09|29883\n",
    "venkatkotha|23149\n",
    "```\n",
    "\n",
    "\n",
    "**Number of users with single post**\n",
    "```\n",
    "sqlite> select count(*) from (select  users.user, count(*)\n",
    "   ...> from (select user from nodes\n",
    "   ...> union all\n",
    "   ...> select user from ways) users\n",
    "   ...> group by users.user having count(*) ==1) ;\n",
    "90\n",
    "```\n",
    "\n",
    "**Number of Dominos**\n",
    "```\n",
    "sqlite> select count(*) from nodes_tags where value Like \"domino%\";\n",
    "3\n",
    "```\n",
    "\n",
    "**Number of Dominos**\n",
    "```\n",
    "sqlite> select count(*) from nodes_tags where value Like \"kfc_\";\n",
    "1\n",
    "```\n",
    "\n",
    "**Banks in South Delhi Region**\n",
    "```\n",
    "sqlite> select nt.value, count(*) \n",
    "   ...> from nodes_tags as nt\n",
    "   ...> join\n",
    "   ...> ( select distinct(id)\n",
    "   ...> from nodes_tags\n",
    "   ...> where value ='bank') bank\n",
    "   ...> on nt.id = bank.id\n",
    "   ...> where nt.key = 'name'\n",
    "   ...> group by nt.value\n",
    "   ...> order by count(*) DESC;\n",
    "   \n",
    "Andhra Bank|1\n",
    "CITI Bank|1\n",
    "HDFC|1\n",
    "HDFC Bank|1\n",
    "ICICI Bank|1\n",
    "ICICI, SBI, Citibank,|1\n",
    "IDBI Bank|1\n",
    "Indian Bank|1\n",
    "Punjab National Bank|1\n",
    "Standard Chartered Bank|1\n",
    "State Bank of India|1   \n",
    "```\n",
    "\n",
    "\n",
    "**Number of Bus Stops**\n",
    "```\n",
    "sqlite> select count(*)\n",
    "   ...> from nodes_tags\n",
    "   ...> where key=\"highway\" and value =\"bus_stop\";\n",
    "69\n",
    "```\n",
    "\n",
    "**Number of Traffic Signals**\n",
    "```\n",
    "sqlite> select count(*)\n",
    "   ...> from nodes_tags\n",
    "   ...> where key=\"highway\" and value =\"traffic_signals\";\n",
    "150\n",
    "```\n",
    "\n",
    "**Cusines in South Delhi Resturants**\n",
    "```\n",
    "sqlite> select nt.value, count(*)\n",
    "   ...> from nodes_tags as nt\n",
    "   ...> join\n",
    "   ...> (select distinct(id)\n",
    "   ...> from nodes_tags\n",
    "   ...> where value='restaurant') res\n",
    "   ...> on nt.id =res.id\n",
    "   ...> where nt.key = 'cuisine'\n",
    "   ...> group by nt.value\n",
    "   ...> order by count(*) DESC;\n",
    "indian|2\n",
    "regional|2\n",
    "thai|2\n",
    "asian|1\n",
    "pizza|1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "In Delhi(Capital of India) most of the people living here are not native resident of this city. So, there is a huge diversity among people. Here we have done some data wrangling and analysis work on OSM data set of **south delhi** region. \n",
    "\n",
    "### Ideas for improving quality of OSM data \n",
    "> Scripts which may run twice or thrice a day for cleaning and updating the collected data. This will help users to use only well cleaned data. \n",
    "> The data is been updated by millions of people on daily basis, so if it may provide a structured form for all input fields according to a database structure. Then there will be no such requirement of so much cleaning. \n",
    "> OSM can also make this data collection task a game by incentivizing this whole process. Just like the way AWS Mechanical Turk works. Then OSM data will contain only validated data .\n",
    "\n",
    "### Benefits by implementing improvements\n",
    "> Implentation of these changes will result by helping users of Open Street Map with clean data. \n",
    "\n",
    "> Data Scientists will be able to get good insights from the data. These changes will save their majority of time since they will get clean data.\n",
    "\n",
    "### Problems in implementing imporvements\n",
    "> There are some issues that may arise for example writing scripts and running it on large data sets will take time, resources and money. We will need to update our scripts after regular interval of somewhere around 10-15 days. For this specialized team of engineers will be required. \n",
    "> Incentivizing the whole process may lead to good results but money required for incentivizing is a major issue. Here it is required to check whether a real customers is improving the dataset or not. Fraud users and cross validation of data needs to be performed properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "> Udacity Discussion Forums\n",
    "\n",
    "> https://docs.python.org/2/library/re.html\n",
    "\n",
    "> [SQL Tutorial](http://www.w3schools.com/sql/)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
